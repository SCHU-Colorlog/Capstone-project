{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from color_palette import PaletteCreator\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = PaletteCreator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('images/dataset/mean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('images/dataset_3/mean_shuffled_wc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label    1.000000\n",
       "B        0.474724\n",
       "Y        0.473763\n",
       "S        0.469657\n",
       "M        0.293013\n",
       "K        0.116791\n",
       "A        0.097921\n",
       "V       -0.116791\n",
       "L       -0.206575\n",
       "H       -0.225074\n",
       "C             NaN\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = df.corr()\n",
    "corr['label'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['C'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kval\n",
    "df.drop(columns=['A'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H</th>\n",
       "      <th>S</th>\n",
       "      <th>V</th>\n",
       "      <th>L</th>\n",
       "      <th>B</th>\n",
       "      <th>M</th>\n",
       "      <th>Y</th>\n",
       "      <th>K</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.333333</td>\n",
       "      <td>97.666667</td>\n",
       "      <td>157.666667</td>\n",
       "      <td>133.666667</td>\n",
       "      <td>142.666667</td>\n",
       "      <td>70.844404</td>\n",
       "      <td>97.632279</td>\n",
       "      <td>0.381699</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118.666667</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>159.666667</td>\n",
       "      <td>140.666667</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>71.246694</td>\n",
       "      <td>63.307304</td>\n",
       "      <td>0.373856</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.333333</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>139.666667</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>54.558979</td>\n",
       "      <td>71.160311</td>\n",
       "      <td>0.380392</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.666667</td>\n",
       "      <td>88.333333</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>137.666667</td>\n",
       "      <td>74.094046</td>\n",
       "      <td>88.357131</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.666667</td>\n",
       "      <td>68.333333</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>138.666667</td>\n",
       "      <td>53.830104</td>\n",
       "      <td>68.238778</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>6.333333</td>\n",
       "      <td>103.666667</td>\n",
       "      <td>156.666667</td>\n",
       "      <td>128.666667</td>\n",
       "      <td>142.666667</td>\n",
       "      <td>83.067656</td>\n",
       "      <td>103.661457</td>\n",
       "      <td>0.385621</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>60.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>190.333333</td>\n",
       "      <td>167.333333</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>56.608374</td>\n",
       "      <td>56.311671</td>\n",
       "      <td>0.253595</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>156.333333</td>\n",
       "      <td>136.333333</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>57.940908</td>\n",
       "      <td>99.917318</td>\n",
       "      <td>0.386928</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>101.333333</td>\n",
       "      <td>158.666667</td>\n",
       "      <td>131.333333</td>\n",
       "      <td>143.666667</td>\n",
       "      <td>74.628663</td>\n",
       "      <td>101.256745</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>9.333333</td>\n",
       "      <td>87.666667</td>\n",
       "      <td>162.666667</td>\n",
       "      <td>141.666667</td>\n",
       "      <td>141.666667</td>\n",
       "      <td>61.292003</td>\n",
       "      <td>87.810297</td>\n",
       "      <td>0.362092</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>402 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              H           S           V           L           B          M  \\\n",
       "0      8.333333   97.666667  157.666667  133.666667  142.666667  70.844404   \n",
       "1    118.666667   72.000000  159.666667  140.666667  132.000000  71.246694   \n",
       "2      7.333333   71.000000  158.000000  139.666667  138.000000  54.558979   \n",
       "3      4.666667   88.333333  170.000000  145.000000  137.666667  74.094046   \n",
       "4      6.666667   68.333333  177.000000  155.000000  138.666667  53.830104   \n",
       "..          ...         ...         ...         ...         ...        ...   \n",
       "397    6.333333  103.666667  156.666667  128.666667  142.666667  83.067656   \n",
       "398   60.000000   57.000000  190.333333  167.333333  134.000000  56.608374   \n",
       "399   13.000000  100.000000  156.333333  136.333333  145.000000  57.940908   \n",
       "400    8.000000  101.333333  158.666667  131.333333  143.666667  74.628663   \n",
       "401    9.333333   87.666667  162.666667  141.666667  141.666667  61.292003   \n",
       "\n",
       "              Y         K  label  \n",
       "0     97.632279  0.381699      1  \n",
       "1     63.307304  0.373856      0  \n",
       "2     71.160311  0.380392      0  \n",
       "3     88.357131  0.333333      1  \n",
       "4     68.238778  0.305882      1  \n",
       "..          ...       ...    ...  \n",
       "397  103.661457  0.385621      1  \n",
       "398   56.311671  0.253595      0  \n",
       "399   99.917318  0.386928      1  \n",
       "400  101.256745  0.377778      1  \n",
       "401   87.810297  0.362092      0  \n",
       "\n",
       "[402 rows x 9 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['label'])\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7115123456790122"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = cross_val_score(logreg, X, Y, verbose=1, cv=5, scoring='accuracy').mean()\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn1 = KNeighborsClassifier(n_neighbors=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6816049382716051"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = cross_val_score(knn1, X, Y, verbose=1, cv=5, scoring='accuracy').mean()\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd1 = SGDClassifier(loss='hinge', verbose=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 41.71, NNZs: 8, Bias: -9.087102, T: 321, Avg. loss: 12.554406\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 33.73, NNZs: 8, Bias: -1.347439, T: 642, Avg. loss: 10.082637\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 46.20, NNZs: 8, Bias: -1.690389, T: 963, Avg. loss: 9.282107\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 29.65, NNZs: 8, Bias: 7.297193, T: 1284, Avg. loss: 6.874207\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 33.01, NNZs: 8, Bias: -5.334008, T: 1605, Avg. loss: 5.449763\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 37.38, NNZs: 8, Bias: -1.705214, T: 1926, Avg. loss: 5.562918\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 34.59, NNZs: 8, Bias: -1.725273, T: 2247, Avg. loss: 3.965719\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 27.71, NNZs: 8, Bias: -7.281537, T: 2568, Avg. loss: 4.657190\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 30.60, NNZs: 8, Bias: -4.169337, T: 2889, Avg. loss: 3.760619\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 29.51, NNZs: 8, Bias: 3.072137, T: 3210, Avg. loss: 3.754121\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 32.01, NNZs: 8, Bias: 4.945197, T: 3531, Avg. loss: 3.393875\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 29.14, NNZs: 8, Bias: 2.553422, T: 3852, Avg. loss: 3.575135\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 28.05, NNZs: 8, Bias: -3.332931, T: 4173, Avg. loss: 3.220131\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 26.26, NNZs: 8, Bias: -4.981437, T: 4494, Avg. loss: 3.023981\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 25.72, NNZs: 8, Bias: 2.115314, T: 4815, Avg. loss: 2.756117\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 24.54, NNZs: 8, Bias: -1.338763, T: 5136, Avg. loss: 2.896052\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 24.27, NNZs: 8, Bias: -4.343917, T: 5457, Avg. loss: 2.601595\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 24.28, NNZs: 8, Bias: 3.185591, T: 5778, Avg. loss: 2.173272\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 24.21, NNZs: 8, Bias: -1.210841, T: 6099, Avg. loss: 2.312972\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 23.60, NNZs: 8, Bias: -1.187441, T: 6420, Avg. loss: 2.226432\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 21.90, NNZs: 8, Bias: 2.721994, T: 6741, Avg. loss: 1.881886\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 21.09, NNZs: 8, Bias: 0.093722, T: 7062, Avg. loss: 2.195233\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 20.17, NNZs: 8, Bias: -2.344233, T: 7383, Avg. loss: 1.891688\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 19.96, NNZs: 8, Bias: -1.108867, T: 7704, Avg. loss: 2.259353\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 18.11, NNZs: 8, Bias: -2.200698, T: 8025, Avg. loss: 2.025637\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 18.99, NNZs: 8, Bias: 1.068034, T: 8346, Avg. loss: 1.714145\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 18.84, NNZs: 8, Bias: 1.027918, T: 8667, Avg. loss: 1.773473\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 17.96, NNZs: 8, Bias: -4.015871, T: 8988, Avg. loss: 1.978522\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 17.70, NNZs: 8, Bias: -0.993731, T: 9309, Avg. loss: 2.015623\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 17.95, NNZs: 8, Bias: -0.035297, T: 9630, Avg. loss: 1.834625\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 17.93, NNZs: 8, Bias: -0.985631, T: 9951, Avg. loss: 1.596218\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 17.28, NNZs: 8, Bias: -0.084253, T: 10272, Avg. loss: 1.700702\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 17.44, NNZs: 8, Bias: -0.961114, T: 10593, Avg. loss: 1.449289\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 16.42, NNZs: 8, Bias: 0.729103, T: 10914, Avg. loss: 1.716533\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 17.02, NNZs: 8, Bias: 0.701179, T: 11235, Avg. loss: 1.610795\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 15.87, NNZs: 8, Bias: 0.673914, T: 11556, Avg. loss: 1.716731\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 16.47, NNZs: 8, Bias: -3.243989, T: 11877, Avg. loss: 1.497653\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 16.15, NNZs: 8, Bias: -0.145327, T: 12198, Avg. loss: 1.758920\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 38 epochs took 0.00 seconds\n",
      "-- Epoch 1\n",
      "Norm: 24.71, NNZs: 8, Bias: -8.801036, T: 321, Avg. loss: 13.875366\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 27.30, NNZs: 8, Bias: -7.687662, T: 642, Avg. loss: 9.942050\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 24.28, NNZs: 8, Bias: 13.175065, T: 963, Avg. loss: 8.593481\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 23.37, NNZs: 8, Bias: -10.354819, T: 1284, Avg. loss: 7.240406\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 26.13, NNZs: 8, Bias: -1.609770, T: 1605, Avg. loss: 5.807928\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 26.86, NNZs: 8, Bias: -1.406714, T: 1926, Avg. loss: 6.128637\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 26.62, NNZs: 8, Bias: 1.643261, T: 2247, Avg. loss: 4.749221\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 26.04, NNZs: 8, Bias: -6.885553, T: 2568, Avg. loss: 4.688474\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 22.74, NNZs: 8, Bias: -3.999730, T: 2889, Avg. loss: 4.142258\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 20.47, NNZs: 8, Bias: 1.007375, T: 3210, Avg. loss: 4.000813\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 20.53, NNZs: 8, Bias: 2.959193, T: 3531, Avg. loss: 3.564569\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 17.02, NNZs: 8, Bias: 0.666516, T: 3852, Avg. loss: 3.837067\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 15.33, NNZs: 8, Bias: -5.263696, T: 4173, Avg. loss: 3.260843\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 16.49, NNZs: 8, Bias: -1.375763, T: 4494, Avg. loss: 3.301537\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 18.12, NNZs: 8, Bias: 3.859930, T: 4815, Avg. loss: 2.952390\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 17.88, NNZs: 8, Bias: -2.941918, T: 5136, Avg. loss: 2.577639\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 13.72, NNZs: 8, Bias: -2.800332, T: 5457, Avg. loss: 2.511261\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 15.65, NNZs: 8, Bias: 1.731775, T: 5778, Avg. loss: 2.340918\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 14.88, NNZs: 8, Bias: -2.586931, T: 6099, Avg. loss: 2.411601\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 15.10, NNZs: 8, Bias: 0.187482, T: 6420, Avg. loss: 2.615719\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 14.16, NNZs: 8, Bias: 0.188719, T: 6741, Avg. loss: 2.465290\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 14.75, NNZs: 8, Bias: -1.114476, T: 7062, Avg. loss: 2.426778\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 13.69, NNZs: 8, Bias: 1.322127, T: 7383, Avg. loss: 1.945949\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 12.93, NNZs: 8, Bias: 4.712600, T: 7704, Avg. loss: 2.190506\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 11.29, NNZs: 8, Bias: -2.113945, T: 8025, Avg. loss: 2.294900\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 12.21, NNZs: 8, Bias: -0.971937, T: 8346, Avg. loss: 1.849020\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 13.80, NNZs: 8, Bias: 1.131575, T: 8667, Avg. loss: 1.748453\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 11.49, NNZs: 8, Bias: -1.911725, T: 8988, Avg. loss: 2.085425\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 11.06, NNZs: 8, Bias: 0.102611, T: 9309, Avg. loss: 1.979367\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 9.92, NNZs: 8, Bias: 0.096140, T: 9630, Avg. loss: 1.979150\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 9.65, NNZs: 8, Bias: -0.826895, T: 9951, Avg. loss: 1.789513\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 8.64, NNZs: 8, Bias: 0.952205, T: 10272, Avg. loss: 1.865332\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 32 epochs took 0.00 seconds\n",
      "-- Epoch 1\n",
      "Norm: 25.52, NNZs: 8, Bias: -16.332630, T: 322, Avg. loss: 12.465327\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 33.94, NNZs: 8, Bias: 10.310769, T: 644, Avg. loss: 11.199525\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 45.36, NNZs: 8, Bias: -2.037654, T: 966, Avg. loss: 8.417439\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 39.55, NNZs: 8, Bias: -15.343568, T: 1288, Avg. loss: 8.559946\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 42.72, NNZs: 8, Bias: -17.705270, T: 1610, Avg. loss: 6.020264\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 35.83, NNZs: 8, Bias: 11.293457, T: 1932, Avg. loss: 4.698421\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 29.13, NNZs: 8, Bias: 0.692521, T: 2254, Avg. loss: 4.872937\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 29.20, NNZs: 8, Bias: 0.657629, T: 2576, Avg. loss: 4.585989\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 26.87, NNZs: 8, Bias: -4.568325, T: 2898, Avg. loss: 4.243233\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 30.06, NNZs: 8, Bias: -1.990387, T: 3220, Avg. loss: 3.999360\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 25.69, NNZs: 8, Bias: -4.041560, T: 3542, Avg. loss: 3.643837\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 24.31, NNZs: 8, Bias: 4.451579, T: 3864, Avg. loss: 3.822575\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 23.77, NNZs: 8, Bias: -3.535978, T: 4186, Avg. loss: 3.450642\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 22.97, NNZs: 8, Bias: -3.404597, T: 4508, Avg. loss: 3.189582\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 20.16, NNZs: 8, Bias: 0.212716, T: 4830, Avg. loss: 3.151899\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 19.30, NNZs: 8, Bias: 3.425314, T: 5152, Avg. loss: 3.108012\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 17.51, NNZs: 8, Bias: -1.417981, T: 5474, Avg. loss: 2.798050\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 19.66, NNZs: 8, Bias: 6.011594, T: 5796, Avg. loss: 3.025089\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 21.12, NNZs: 8, Bias: -2.700903, T: 6118, Avg. loss: 2.394944\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 21.74, NNZs: 8, Bias: 2.813127, T: 6440, Avg. loss: 2.587057\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 21.01, NNZs: 8, Bias: 1.383265, T: 6762, Avg. loss: 2.341886\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 21.88, NNZs: 8, Bias: -2.330466, T: 7084, Avg. loss: 2.528177\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 21.91, NNZs: 8, Bias: 3.713072, T: 7406, Avg. loss: 2.229983\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 20.48, NNZs: 8, Bias: 0.117243, T: 7728, Avg. loss: 2.343364\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 19.97, NNZs: 8, Bias: -5.438574, T: 8050, Avg. loss: 2.291749\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 18.16, NNZs: 8, Bias: 0.087147, T: 8372, Avg. loss: 2.215427\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 17.15, NNZs: 8, Bias: -1.960469, T: 8694, Avg. loss: 1.823925\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 17.06, NNZs: 8, Bias: 1.087394, T: 9016, Avg. loss: 2.131936\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 16.33, NNZs: 8, Bias: 0.080498, T: 9338, Avg. loss: 1.855975\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 16.79, NNZs: 8, Bias: 1.955919, T: 9660, Avg. loss: 1.534853\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 16.14, NNZs: 8, Bias: -2.655058, T: 9982, Avg. loss: 2.050516\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 15.08, NNZs: 8, Bias: -1.720164, T: 10304, Avg. loss: 2.030626\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 15.16, NNZs: 8, Bias: 0.891567, T: 10626, Avg. loss: 1.777261\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 16.20, NNZs: 8, Bias: -0.789203, T: 10948, Avg. loss: 1.918295\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 16.33, NNZs: 8, Bias: 1.669053, T: 11270, Avg. loss: 1.629601\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 35 epochs took 0.00 seconds\n",
      "-- Epoch 1\n",
      "Norm: 28.68, NNZs: 8, Bias: -22.987015, T: 322, Avg. loss: 13.839236\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 41.45, NNZs: 8, Bias: -12.867187, T: 644, Avg. loss: 10.409584\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 34.18, NNZs: 8, Bias: -0.890493, T: 966, Avg. loss: 8.309862\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 29.01, NNZs: 8, Bias: -9.542515, T: 1288, Avg. loss: 6.640256\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 36.05, NNZs: 8, Bias: -12.680901, T: 1610, Avg. loss: 5.409399\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 29.47, NNZs: 8, Bias: 2.224934, T: 1932, Avg. loss: 5.188777\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 31.96, NNZs: 8, Bias: -1.123651, T: 2254, Avg. loss: 4.415782\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 23.04, NNZs: 8, Bias: 4.554336, T: 2576, Avg. loss: 4.195484\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 23.66, NNZs: 8, Bias: -0.969142, T: 2898, Avg. loss: 4.000196\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 20.67, NNZs: 8, Bias: -3.384249, T: 3220, Avg. loss: 3.551653\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 22.40, NNZs: 8, Bias: -1.148453, T: 3542, Avg. loss: 2.923592\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 21.87, NNZs: 8, Bias: 3.044479, T: 3864, Avg. loss: 3.577034\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 20.87, NNZs: 8, Bias: 2.802705, T: 4186, Avg. loss: 3.188299\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 21.84, NNZs: 8, Bias: -2.825218, T: 4508, Avg. loss: 2.759100\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 20.38, NNZs: 8, Bias: 2.383939, T: 4830, Avg. loss: 2.906687\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 19.26, NNZs: 8, Bias: 3.862639, T: 5152, Avg. loss: 2.860930\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 17.36, NNZs: 8, Bias: -4.084023, T: 5474, Avg. loss: 2.525523\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 15.91, NNZs: 8, Bias: 1.943490, T: 5796, Avg. loss: 2.415948\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 17.32, NNZs: 8, Bias: -2.362039, T: 6118, Avg. loss: 2.438754\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 18.13, NNZs: 8, Bias: -2.290818, T: 6440, Avg. loss: 2.245576\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 16.95, NNZs: 8, Bias: 0.341994, T: 6762, Avg. loss: 2.453891\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 16.23, NNZs: 8, Bias: -2.159678, T: 7084, Avg. loss: 2.407213\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 15.95, NNZs: 8, Bias: 0.273772, T: 7406, Avg. loss: 2.014955\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 16.90, NNZs: 8, Bias: -2.048575, T: 7728, Avg. loss: 2.020502\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 15.86, NNZs: 8, Bias: -0.872110, T: 8050, Avg. loss: 2.048390\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 14.43, NNZs: 8, Bias: -1.920359, T: 8372, Avg. loss: 1.880598\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 14.67, NNZs: 8, Bias: -3.935342, T: 8694, Avg. loss: 1.814070\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 15.02, NNZs: 8, Bias: -1.827419, T: 9016, Avg. loss: 1.808459\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 13.00, NNZs: 8, Bias: 0.143234, T: 9338, Avg. loss: 1.905432\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 13.26, NNZs: 8, Bias: 0.136333, T: 9660, Avg. loss: 1.615062\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 15.16, NNZs: 8, Bias: -1.697226, T: 9982, Avg. loss: 1.820975\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 13.89, NNZs: 8, Bias: -0.785018, T: 10304, Avg. loss: 1.794133\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 12.29, NNZs: 8, Bias: 1.796417, T: 10626, Avg. loss: 1.513360\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 12.97, NNZs: 8, Bias: -0.748407, T: 10948, Avg. loss: 1.623177\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 13.27, NNZs: 8, Bias: 0.071634, T: 11270, Avg. loss: 1.471599\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 12.46, NNZs: 8, Bias: -2.325920, T: 11592, Avg. loss: 1.560970\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 10.88, NNZs: 8, Bias: 0.814277, T: 11914, Avg. loss: 1.484656\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 11.59, NNZs: 8, Bias: 0.026205, T: 12236, Avg. loss: 1.597100\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 11.90, NNZs: 8, Bias: 1.502325, T: 12558, Avg. loss: 1.489116\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 11.55, NNZs: 8, Bias: -0.710121, T: 12880, Avg. loss: 1.404358\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 11.11, NNZs: 8, Bias: 2.124228, T: 13202, Avg. loss: 1.435115\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 12.41, NNZs: 8, Bias: -0.668927, T: 13524, Avg. loss: 1.469282\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 11.72, NNZs: 8, Bias: -1.331576, T: 13846, Avg. loss: 1.515179\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 11.46, NNZs: 8, Bias: -1.974739, T: 14168, Avg. loss: 1.237503\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 12.12, NNZs: 8, Bias: -0.643047, T: 14490, Avg. loss: 1.236110\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 11.53, NNZs: 8, Bias: 0.627170, T: 14812, Avg. loss: 1.300083\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 11.24, NNZs: 8, Bias: -0.009792, T: 15134, Avg. loss: 1.278249\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 11.55, NNZs: 8, Bias: 0.595366, T: 15456, Avg. loss: 1.254125\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 11.74, NNZs: 8, Bias: -1.204840, T: 15778, Avg. loss: 1.406111\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 11.04, NNZs: 8, Bias: -1.773152, T: 16100, Avg. loss: 1.190537\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 10.30, NNZs: 8, Bias: -0.604990, T: 16422, Avg. loss: 1.296014\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 10.14, NNZs: 8, Bias: 0.536576, T: 16744, Avg. loss: 1.257776\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 10.19, NNZs: 8, Bias: -0.032151, T: 17066, Avg. loss: 1.294863\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 9.64, NNZs: 8, Bias: -0.035396, T: 17388, Avg. loss: 1.151466\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 9.73, NNZs: 8, Bias: -0.570349, T: 17710, Avg. loss: 1.208002\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 8.82, NNZs: 8, Bias: 0.486749, T: 18032, Avg. loss: 1.154953\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 8.37, NNZs: 8, Bias: -0.559508, T: 18354, Avg. loss: 1.294201\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 9.08, NNZs: 8, Bias: 0.977663, T: 18676, Avg. loss: 1.192031\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 9.05, NNZs: 8, Bias: 0.456348, T: 18998, Avg. loss: 1.283029\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 59 epochs took 0.00 seconds\n",
      "-- Epoch 1\n",
      "Norm: 24.70, NNZs: 8, Bias: 0.531347, T: 322, Avg. loss: 13.720331\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 38.46, NNZs: 8, Bias: -11.534929, T: 644, Avg. loss: 10.761597\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 27.47, NNZs: 8, Bias: 0.097319, T: 966, Avg. loss: 8.978771\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 36.85, NNZs: 8, Bias: -17.281136, T: 1288, Avg. loss: 7.325560\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 28.93, NNZs: 8, Bias: -15.184439, T: 1610, Avg. loss: 6.142217\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 32.19, NNZs: 8, Bias: -3.390963, T: 1932, Avg. loss: 5.593568\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 26.15, NNZs: 8, Bias: -0.156248, T: 2254, Avg. loss: 5.508599\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 28.81, NNZs: 8, Bias: 5.528350, T: 2576, Avg. loss: 4.174257\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 28.89, NNZs: 8, Bias: 2.480232, T: 2898, Avg. loss: 4.551745\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 27.79, NNZs: 8, Bias: -2.458810, T: 3220, Avg. loss: 3.658424\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 25.13, NNZs: 8, Bias: -0.015768, T: 3542, Avg. loss: 3.599630\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 26.04, NNZs: 8, Bias: 2.051119, T: 3864, Avg. loss: 3.413851\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 24.99, NNZs: 8, Bias: 0.024544, T: 4186, Avg. loss: 3.534610\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 27.01, NNZs: 8, Bias: 1.842550, T: 4508, Avg. loss: 3.142980\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 25.18, NNZs: 8, Bias: 1.708596, T: 4830, Avg. loss: 2.895793\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 24.35, NNZs: 8, Bias: 4.901065, T: 5152, Avg. loss: 3.220740\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 22.26, NNZs: 8, Bias: 0.042463, T: 5474, Avg. loss: 3.106661\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 22.74, NNZs: 8, Bias: 0.030544, T: 5796, Avg. loss: 2.785629\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 19.93, NNZs: 8, Bias: -0.016911, T: 6118, Avg. loss: 2.616131\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 20.51, NNZs: 8, Bias: -1.367432, T: 6440, Avg. loss: 2.397190\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 19.37, NNZs: 8, Bias: -1.340382, T: 6762, Avg. loss: 2.442018\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 19.72, NNZs: 8, Bias: -1.300041, T: 7084, Avg. loss: 2.166802\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 20.23, NNZs: 8, Bias: 3.514245, T: 7406, Avg. loss: 2.418420\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 19.52, NNZs: 8, Bias: -0.048246, T: 7728, Avg. loss: 2.260485\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 18.53, NNZs: 8, Bias: -2.254513, T: 8050, Avg. loss: 2.176865\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 18.18, NNZs: 8, Bias: -3.276457, T: 8372, Avg. loss: 2.001593\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 17.32, NNZs: 8, Bias: 0.943314, T: 8694, Avg. loss: 1.957472\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 17.49, NNZs: 8, Bias: -1.078355, T: 9016, Avg. loss: 2.014293\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 17.82, NNZs: 8, Bias: 1.859045, T: 9338, Avg. loss: 1.833650\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 16.55, NNZs: 8, Bias: 2.755001, T: 9660, Avg. loss: 2.031396\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 16.74, NNZs: 8, Bias: -0.961901, T: 9982, Avg. loss: 1.876343\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 17.35, NNZs: 8, Bias: -0.060099, T: 10304, Avg. loss: 1.717564\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 17.99, NNZs: 8, Bias: -1.792156, T: 10626, Avg. loss: 1.754241\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 16.53, NNZs: 8, Bias: -0.064814, T: 10948, Avg. loss: 1.882088\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 16.20, NNZs: 8, Bias: -0.876119, T: 11270, Avg. loss: 1.716753\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 16.10, NNZs: 8, Bias: -0.071308, T: 11592, Avg. loss: 1.713510\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 15.86, NNZs: 8, Bias: -0.081150, T: 11914, Avg. loss: 1.681982\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 16.10, NNZs: 8, Bias: -0.825398, T: 12236, Avg. loss: 1.526141\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 16.25, NNZs: 8, Bias: 0.676732, T: 12558, Avg. loss: 1.629670\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 15.57, NNZs: 8, Bias: -0.060478, T: 12880, Avg. loss: 1.637989\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 15.41, NNZs: 8, Bias: 2.058334, T: 13202, Avg. loss: 1.603017\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 15.42, NNZs: 8, Bias: 0.640706, T: 13524, Avg. loss: 1.560370\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 15.61, NNZs: 8, Bias: -0.712553, T: 13846, Avg. loss: 1.441145\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 14.92, NNZs: 8, Bias: -1.367627, T: 14168, Avg. loss: 1.447428\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 14.99, NNZs: 8, Bias: -0.046298, T: 14490, Avg. loss: 1.409340\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 14.99, NNZs: 8, Bias: -0.673978, T: 14812, Avg. loss: 1.361687\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 15.35, NNZs: 8, Bias: -0.664822, T: 15134, Avg. loss: 1.435900\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 15.16, NNZs: 8, Bias: -1.263514, T: 15456, Avg. loss: 1.458619\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 14.93, NNZs: 8, Bias: -0.651824, T: 15778, Avg. loss: 1.448382\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 14.06, NNZs: 8, Bias: -0.641649, T: 16100, Avg. loss: 1.329079\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 13.94, NNZs: 8, Bias: -1.781514, T: 16422, Avg. loss: 1.454521\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 13.92, NNZs: 8, Bias: 0.504704, T: 16744, Avg. loss: 1.452311\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 13.76, NNZs: 8, Bias: -0.053385, T: 17066, Avg. loss: 1.517050\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 13.59, NNZs: 8, Bias: 0.492506, T: 17388, Avg. loss: 1.376608\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 13.40, NNZs: 8, Bias: -0.054383, T: 17710, Avg. loss: 1.281867\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 12.78, NNZs: 8, Bias: -0.581164, T: 18032, Avg. loss: 1.177165\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 12.70, NNZs: 8, Bias: 0.985685, T: 18354, Avg. loss: 1.305079\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 12.71, NNZs: 8, Bias: 0.971819, T: 18676, Avg. loss: 1.213666\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 12.78, NNZs: 8, Bias: 1.460956, T: 18998, Avg. loss: 1.296781\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 12.44, NNZs: 8, Bias: 0.942886, T: 19320, Avg. loss: 1.364680\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 13.11, NNZs: 8, Bias: 0.444278, T: 19642, Avg. loss: 1.223916\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 61 epochs took 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6816358024691358"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = cross_val_score(sgd1, X, Y, verbose=1, cv=5, scoring='accuracy').mean()\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = ['linear', 'poly', 'sigmoid', 'rbf']\n",
    "gamma = [0.01, 0.1, 1, 10, 50, 100, 'auto', 'scale']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear 0.1 0.6941666666666666\n",
      "linear 1 0.7065432098765433\n",
      "linear 10 0.7139506172839507\n",
      "linear 100 0.7064506172839506\n",
      "poly 0.1 0.6693827160493827\n",
      "poly 1 0.6867592592592592\n",
      "poly 10 0.6618518518518519\n",
      "poly 100 0.6493518518518518\n",
      "sigmoid 0.1 0.6839814814814815\n",
      "sigmoid 1 0.5399074074074074\n",
      "sigmoid 10 0.5473456790123457\n",
      "sigmoid 100 0.5473456790123457\n",
      "rbf 0.1 0.6890740740740741\n",
      "rbf 1 0.6966049382716049\n",
      "rbf 10 0.6991049382716049\n",
      "rbf 100 0.6442901234567902\n"
     ]
    }
   ],
   "source": [
    "for k in kernel:\n",
    "    for c in [0.1, 1, 10, 100]:\n",
    "        svc = SVC(kernel=k, C=c, probability=True, random_state=42)\n",
    "        score = cross_val_score(svc, X, Y, cv=5, scoring='accuracy').mean()\n",
    "        print(f'{k} {c} {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in kernel:\n",
    "    for c in range(1, 9):\n",
    "        for g in gamma:\n",
    "            svc = SVC(kernel=k, C=(c), probability=True, gamma=g, random_state=42)\n",
    "            score = cross_val_score(svc, X, Y, cv=5, scoring='accuracy').mean()\n",
    "            print(f'{k} {c} {g} {score}')\n",
    "\n",
    "            svc = SVC(kernel=k, C=(c * .1), probability=True, gamma=g, random_state=42)\n",
    "            score = cross_val_score(svc, X, Y, cv=5, scoring='accuracy').mean()\n",
    "            print(f'{k} {c} {g} {score}')\n",
    "\n",
    "            svc = SVC(kernel=k, C=(c * 10), probability=True, gamma=g, random_state=42)\n",
    "            score = cross_val_score(svc, X, Y, cv=5, scoring='accuracy').mean()\n",
    "            print(f'{k} {c} {g} {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [10, 50, 100, 200, 500]\n",
    "criterion = ['gini', 'entropy', 'log_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ne in n_estimators:\n",
    "    for c in criterion:\n",
    "        forest = RandomForestClassifier(n_estimators=ne, criterion=c, random_state=42)\n",
    "        score = cross_val_score(forest, X, Y, cv=5, scoring='accuracy').mean()\n",
    "        print(f'{ne} {c} {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_svc = SVC(kernel='linear', C=7, probability=True, gamma=1, random_state=42)\n",
    "second_best_svc = SVC(kernel='linear', C=10, probability=True, random_state=42)\n",
    "sgd = SGDClassifier(loss='epsilon_insensitive', penalty=None, alpha=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf = VotingClassifier(estimators=[('svc1', best_svc),\n",
    "                                          ('svc2', second_best_svc),\n",
    "                                          ('sgd', sgd)],\n",
    "                              voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cross_val_score(voting_clf, X, Y, cv=5, scoring='accuracy').mean()\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf = VotingClassifier(estimators=[('svc1', best_svc),\n",
    "                                          ('svc2', second_best_svc)],\n",
    "                              voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cross_val_score(voting_clf, X, Y, cv=5, scoring='accuracy').mean()\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf = VotingClassifier(estimators=[('svc1', best_svc),\n",
    "                                          ('svc2', second_best_svc)],\n",
    "                                  voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cross_val_score(voting_clf, X, Y, cv=5, scoring='accuracy').mean()\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = ['hinge', 'log_loss', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_error', 'epsilon_insensitive', 'squared_epsilon_insensitive']\n",
    "penalty = ['l1', 'l2', 'elasticnet', None]\n",
    "alpha = [0.0001, 0.001, 0.01, 0.1, 1, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in loss:\n",
    "    for p in penalty:\n",
    "        for a in alpha:\n",
    "            sgd = SGDClassifier(loss=l, penalty=p, alpha=a, random_state=42)\n",
    "            score = cross_val_score(sgd, X, Y, cv=5, scoring='accuracy').mean()\n",
    "            print(f'{l} {p} {a} {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
